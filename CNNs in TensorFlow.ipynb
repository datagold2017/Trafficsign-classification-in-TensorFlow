{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical introduction to convolutional neural networks (CNNs) in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi guys! My name Robert. I'm currently a student at OTH Regensburg. Due to a 2 months project work I had my first hands on experience with python, TensorFlow and convolutional neural networks. This notebook and the code that goes with it are the results of that work. This Notebook is supposed to help sutdents getting started with TensorFlow and give some insights into CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we going to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<> Build a CNN for traffic sign classification with ~ 99.1 % accuracy based on the GTSRB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<> Use TensorFlow and Python to do all the work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<> Get a feeling on how CNNs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<> What to keep in mind when working with a CNN"
   ]
  },
  {
   "cell_type": "
   ",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<> TensorFlow with GPU support (and a supported GPU!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<> 16 GB RAM (since we will do all the stuff quick and dirty ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<> skimage and OpenCV for python 3.5.x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are a pretty good technology for image classification tasks. To have a more real life like feeling (looking at you MNIST) we are going to build a traffic sign classifier based on the German Traffic Sign Recognition Benchmark (GTSRB) dataset. You can get the [raw - data](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at what we are dealing with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Resources/Raw_images_every_class.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are in between 15x15 and 250x250 pixel with varying aspect ratios. Mean height and width are ~50.1 pixel. The trainig set has a moderate size with around 32.000 samples. It's not too bad but we should augument it a bit. We are also going to fix the aspect ratio problem first by resizing to 48x48 pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Augumenting functions based on Naoki Shibuya's work! Thank you!\n",
    "## https://github.com/naokishibuya/car-traffic-sign-classification\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resizeImage(image):\n",
    "    return cv2.resize(img, (48,48))\n",
    "\n",
    "\n",
    "def random_brightness(image, ratio):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    brightness = np.float64(hsv[:, :, 2])\n",
    "    brightness = brightness * (1.0 + np.random.uniform(-ratio, ratio))\n",
    "    brightness[brightness>255] = 255\n",
    "    brightness[brightness<0] = 0\n",
    "    hsv[:, :, 2] = brightness\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "\n",
    "def random_rotation(image, angle):\n",
    "    if angle == 0:\n",
    "        return image\n",
    "    angle = np.random.uniform(-angle, angle)\n",
    "    rows, cols = image.shape[:2]\n",
    "    size = cols, rows\n",
    "    center = cols/2, rows/2\n",
    "    scale = 1.0\n",
    "    rotation = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    return cv2.warpAffine(image, rotation, size)\n",
    "\n",
    "\n",
    "def random_translation(image, translation):\n",
    "    if translation == 0:\n",
    "        return 0\n",
    "    rows, cols = image.shape[:2]\n",
    "    size = cols, rows\n",
    "    x = np.random.uniform(-translation, translation)\n",
    "    y = np.random.uniform(-translation, translation)\n",
    "    trans = np.float32([[1,0,x],[0,1,y]])\n",
    "    return cv2.warpAffine(image, trans, size)\n",
    "\n",
    "\n",
    "def random_shear(image, shear):\n",
    "    if shear == 0:\n",
    "        return image\n",
    "    rows, cols = image.shape[:2]\n",
    "    size = cols, rows\n",
    "    left, right, top, bottom = shear, cols - shear, shear, rows - shear\n",
    "    dx = np.random.uniform(-shear, shear)\n",
    "    dy = np.random.uniform(-shear, shear)\n",
    "    p1 = np.float32([[left   , top],[right   , top   ],[left, bottom]])\n",
    "    p2 = np.float32([[left+dx, top],[right+dx, top+dy],[left, bottom+dy]])\n",
    "    move = cv2.getAffineTransform(p1,p2)\n",
    "    return cv2.warpAffine(image, move, size)\n",
    "\n",
    "\n",
    "\n",
    "def augment_image(image, brightness, angle, translation, shear):\n",
    "    image = resizeImage(image)\n",
    "    aug_images = []\n",
    "    aug_images.append(image)\n",
    "    aug_images.append(random_brightness(image, brightness))\n",
    "    aug_images.append(random_rotation(image, angle))\n",
    "    aug_images.append(random_translation(image, translation))\n",
    "    aug_images.append(random_shear(image, shear))\n",
    "    aug_images.append(random_rotation(random_brightness(image, brightness), angle))\n",
    "    aug_images.append(random_translation(random_brightness(image, brightness), translation))\n",
    "    aug_images.append(random_shear(random_brightness(image, brightness), shear))\n",
    "    return aug_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have expanded our dataset by factor 8. Let's look at the distribution of datapoints over all classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Resources//Distribution_enh_skewed.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with a skewed dataset like this isn't a big problem. But since we want to be sure not to learn the distribution we are going to split our set in a skewed and an equalized set with 200 random samples per class. We will use the equalized set for a second training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "## our expanded dataset is stored in\n",
    "enh_images, enh_labels = (...)\n",
    "\n",
    "#since we need to know the start and end of the current class\n",
    "lower_bound = 0\n",
    "upper_bound = len(enh_images)\n",
    "\n",
    "eq_images = []\n",
    "eq_labels = []\n",
    "\n",
    "#assumtion: class indizes are sorted and increasing\n",
    "for classindex in range(43):\n",
    "\tfor position in range(lower_bound, upper_bound):\n",
    "\t\tif enh_labels[position] != classindex:\n",
    "\t\t\tupper_bound\t= position\n",
    "\t\t\tbreak\n",
    "\tsample_index= random.sample(range(lower_bound, upper_bound), 200)  \n",
    "\n",
    "\tfor i in sample_index:\n",
    "\t\teq_images.append(enh_images[i][:])\n",
    "\n",
    "\tfor i in sample_index:\n",
    "\t\teq_labels.append(enh_labels[i])\n",
    "\n",
    "\tcnt = 0\n",
    "\tfor i in sample_index:\n",
    "\t\tenh_images.pop(i - cnt)\n",
    "\t\tenh_labels.pop(i - cnt)\n",
    "\t\tcnt +=1\n",
    "        \n",
    "\tlower_bound = upper_bound\n",
    "\tupper_bound = len(enh_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All preprocessing steps can be found in [ xyz .py ]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the TensorFlow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At the beginnig I promised a final accuracy of ~99.1%. For this we are going to use a network with 10 Layers and a ResNet-like extension. But be !! WARNED !! training that one on a Nvidia GTX 1050Ti takes around 50 hours. The final network can be found in [ xyz .py ]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will just go with the general structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| In |                      RGB 48x48                     |\n",
    "|----|:--------------------------------------------------:|\n",
    "| 1  | Conv 5x5, 96 / ReLU -> Local Resp. Norm. -> Max Pool 2x2 |\n",
    "| 2  | Conv 3x3, 96 / ReLU -> Local Resp. Norm. -> Max Pool 2x2 |\n",
    "| 3  |                 Conv 3x3, 48 / ReLU                |\n",
    "| 4  |                 Conv 3x3, 48 / ReLU                |\n",
    "| 5  | Conv 3x3, 48 / ReLU  -> Local Resp. Norm. -> Max Pool 2x2 |\n",
    "| 6  | Conv 2x2, 48 / ReLU -> Local Resp. Norm. -> Max Pool 2x2 |\n",
    "| 7  |          FC Layer 3+6, 2048 / ReLU -> Dropout         |\n",
    "| 8  |           FC Layer 6, 1024 / ReLU -> Dropout          |\n",
    "| 9  |          FC Layer 7+8, 768 / ReLU -> Dropout          |\n",
    "| 10 |                        FC 43                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's kinda difficult to find code examples for working with the images-tab in TensorBoard I added a few nice visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage \n",
    "\n",
    "def variable_summaries(var, summary_name):\n",
    "\twith tf.name_scope(\"Summary_\" + str(summary_name)):\n",
    "\t\ttf.summary.scalar(\"raw\", var)\n",
    "\t\ttf.summary.histogram('histogram', var)\n",
    "        \n",
    "def convertToUINT8(x):\n",
    "\treturn skimage.img_as_ubyte(x / np.amax(x))\n",
    "\n",
    "## Based on panmari's post. Thank you!\n",
    "## https://gist.github.com/panmari/4622b78ce21e44e2d69c\n",
    "\n",
    "def VisualizeConvolutions(myTensor, sizeInfo,  name):\n",
    "\tV = tf.slice(myTensor, (0, 0, 0, 0), (1, -1, -1, -1), name='slice_' + name)\n",
    "\tV = tf.reshape(V, (sizeInfo[0], sizeInfo[1], sizeInfo[2]))\n",
    "\tV = tf.transpose(V, (2, 0, 1))\n",
    "\tV = tf.reshape(V, (-1, sizeInfo[0], sizeInfo[1], 1))\n",
    "\ttf.summary.image(name, V,  max_outputs=3)\n",
    "\n",
    "    \n",
    "## Add this in your graph definition to use TensorBoard's \"IMAGE\" Tab\n",
    "## This will print the first 32 feature maps of each layer after the convolution\n",
    "## as well as the original input image\n",
    "\n",
    "with tf.name_scope('Visualiz_convolutions'):\n",
    "    img_raw = tf.slice(x_image, (0, 0, 0, 0), (1, -1, -1, -1), name='slice_raw_image')\n",
    "    tf.summary.image(\"raw_image\", img_raw,  max_outputs=32)\n",
    "\n",
    "    ## note: (width, height, output kernels)\n",
    "    ## 48px = orig. image // 24px = feature map after maxpool \n",
    "    VisualizeConvolutions(h_conv1, [48, 48, 96], \"Conv1\")\n",
    "    VisualizeConvolutions(h_conv2, [24, 24, 96], \"Conv2\")\n",
    "    VisualizeConvolutions(h_conv3, [12, 12, 48], \"Conv3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acutally see something you need to add some lines while running your session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path = r'C:/....'\n",
    "cross_entropy = (...)\n",
    "accuracy = (...)\n",
    "\n",
    "variable_summaries(cross_entropy, 'crossentropy')\n",
    "variable_summaries(accuracy, 'TrainingAcc')\t\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(logs_path)\n",
    "    summary_writer.add_graph(sess.graph)\n",
    "    \n",
    "    #Start training and logging\n",
    "    for i in range(100):\n",
    "        (...)\n",
    "        _, summary = sess.run([train_step, summary_op], feed_dict={(...)})\n",
    "        summary_writer.add_summary(summary, i)\t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding whats going on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of [good websites](http://www.deeplearningbook.org/) out there for learning the mathematical basics. You can also read some papers and find a visualization of the weights or the feature maps after a convolution. But all of this is typically a one time shot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are trained over a long period of time by reviewing the dataset over and over again (1 epoch = every pic has been processed once). While this is done the weights and biases are updated. This changes the outcome of each layers convolution. CNNs also tend to have problems with classifying similar classes. That's why we are going to look at the evolution of our network as the epochs go by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Resources/perClass_acc_epochs_small.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the CNN tends to learn class by class. If you go on an train only for 20 epochs without further thinking you might acutally miss a few classes and get a lower accuracy than you possibly could! Let's have look at the classification behavior (wrong predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Resources/confusion_over_epochs_small.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like our network is having problems with classes 18-31, which is not surprising. All of those are general warning signs with a red outline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok... we are getting better and better as time goes by. That's what we are expecting...but what is actually happening 'inside' of the network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each trainig step a bunch of pics is processed and the weights and biases are updated. The weights are part of the convolution process. So with each iteration the feature maps of each layer slowly change. They learn the neccessary features that are needed to classify the current sign. Let's have a look at the input picture and the first conv. layer output after 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Resources/Conv_Input.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Resources/Convolution_Layer1_Epoche_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It get's way more interesting if we look at this over a view epochs. Also we should look at deeper layers which extract more abstract features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Resources/Evolution_Convs_over_Epochs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok... Let's call it a day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you got a feeling on the learning behavior of a traffic sign classification CNN. Feel free to play aroung with the code or use it on a different dataset. If you got interested and wanna know more about the topic I encourage you to read some of those:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Martín Abadi u. a. TensorFlow: Large-Scale Machine Learning on Heterogeneous Syste Software available from tensorflow.org. 2015. url: http://tensorflow.org/.\n",
    "    \n",
    "[2] Raman Arora u. a. „Understanding Deep Neural Networks with Rectified Linear Units“. In: CoRR abs/1611.01491 (2016). url: http://arxiv.org/abs/1611.01491.\n",
    "\n",
    "[3] Dan C. Ciresan, Ueli Meier und Jürgen Schmidhuber. „Multi-column Deep Neural Networks for Image Classification“. In: CoRR abs/1202.2745 (2012). url: http://arxiv.org/abs/1202.2745.\n",
    "        \n",
    "[4] Alex Krizhevsky, Ilya Sutskever und Geoffrey E Hinton. „ImageNet Classification with Deep Convolutional Neural Networks“. In: Advances in Neural Information Processing Systems 25. Hrsg. von F. Pereira u. a. Curran Associates, Inc., 2012, S. 1097–1105. url: http : / / papers . nips . cc / paper / 4824 - imagenet - classification - with - deep -convolutional-neural-networks.pdf.\n",
    "            \n",
    "[5] Matthew D. Zeiler und Rob Fergus. „Visualizing and Understanding Convolutional Networks“. In: CoRR abs/1311.2901 (2013). url: http://arxiv.org/abs/1311.2901.      "
   ]
  }  
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


